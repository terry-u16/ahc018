# AHC018日記

## 1日目（土）

問題を読んだ。面白い。

AHC003にかなり似ているな。コストを推定しながら最小シュタイナー森（水源を1頂点にまとめればそう考えてよい）を構築する問題。

ただAHC003と違って、事前にボーリング調査をやってしまってよい。ボーリング調査をする際は二分探索をするなり平方分割っぽくやる（100階建てのビルから卵を落とすGoogleの例の問題）なりすれば効率的にできそう。どうせ近くにあるマスのコストはそんなに変わらないので近場いくつか掘ってしまってよい。

ボーリング調査結果からコストを復元するのはどうすればいいだろう。Perlin noiseについて調べてみると、Wavelet関数なるものを格子点上にたくさん作ってそれらを補間してあげているらしい。だけどこれを逆方向に推定できる気がしないな。NN使っていい感じに求められないだろうか。超解像とかimg to imgのネットワークとか。

もしNNで求められるようになれば、仮経路を決める→ボーリング調査→仮経路を決める→ボーリング調査→……の繰り返しで少しずつ精緻化していくのがよい気がする。必ずしも1マスずつ順番に掘り進める必要はない。

NNのタスクを定義する。入出力ともに40x40のグレースケール画像とし、出力側は真のマップをリサイズした画像、入力側は出力側からいくつかの点をサンプリングした画像とする。入力側の欠損値はとりあえずサンプリングされた画像の平均値とする。チャンネルを増やして欠損した・していない情報を付加してもよいが、いったん置いておく。

そういえばパラメータ数は提出可能な範囲に収まっているのだろうか。チャンネル数を無闇に増やさなければ大丈夫だと思ってるけど……。

img to imgということでとりあえずUNetを組んでみる。正直全くノウハウがないので「モザイク除去から学ぶ_最先端のディープラーニング」「Kaggleに挑む深層学習プログラミングの極意」などを読みながら必死にそれっぽいものを作った。python難しすぎ……。パラメータ等はあとでRust移植することを考えず適当。

予測してみる。なんだこれ……思った以上にちゃんと予測できてるぞ……！正直ここまでとは思っていなかったので期待以上。かなりテンションが上がっている。いや本当に凄い……。

チャンネルを増やして欠損した・していない情報を付加した。人間の目で見ても分かるくらいに改善されている。これは凄い。苦労してNNを実装した甲斐があった……。

周波数成分が高いケースと低いケースがあるな。下手するとFFTが使えてしまうかもしれない。[AHCで絶対使わないであろうアルゴリズムリスト](https://twitter.com/terry_u16/status/1626768939094274048)に挙げてたのに……。

信頼区間とかを推定するネットワークがあってもいいかもしれない。メモ。

## 2日目（日）

NNのパラメータ数を減らそうかと思ったけど一旦良いか。パラメータ多すぎかと思ったけど減らすと一応目に見えるレベルで出力が変わるので無駄ではないらしい。とはいえSOTA目指すわけじゃないからあとで実行速度・埋め込み可否と相談かな。

白黒の画像を並べて見比べてたらレントゲン写真を見てるお医者さんの気分になってきた。

NNに必要な部品（Conv2D, BatchNormalization, MaxPoolingなど）を移植する。つらい。とりあえず一つずつ丁寧にテストしていく。

部品まで作成し終わったので、息抜きにNNのoptimizerや学習率スケジューリングをいじってみる。思った以上に効く。知識として知っていても自分で実際に触ってみるのとでは大違いだな。

予測結果を眺めてみる。よく推論できている。何が凄いって、サンプリングされた点が近くになくてもそれらしい傾向を出して来る点が凄い。画像全体の周波数成分から拾ってきているのだろうか。これは人間にはできないな。

低周波成分が強いインスタンスには強い一方、高周波成分が強いインスタンスには弱い傾向がありそう。ナイキスト周波数問題のようなそうでないような。モデルを複数個用意するのもアリかもしれない。

パラメータ数を1桁間違えていた（え？）結構カツカツだな……。うまくバランスを取る必要がありそう。まあ細かい詰めは後で良いか。

高周波成分に対応できるようにするためには、入力で欠損していないピクセルだけ損失関数を大きくしてあげると良いかもしれない。とはいえ低周波には弱くなるからモデル2つ作るのがベターか。エンコード時のFP32をFP16に変えると2倍詰め込めるかも。幸いFP16→FP32の変換はそこまで難しくないらしい。

ようやくネットワークの移植が完了。パーツごとにテストした甲斐があってほぼバグなく結果が出てきた。「ほぼ」というのは結果の小数点第3位とかが微妙に違っているため。batch_normalizationのパラメータかパディングか、なんかそういう細かいところが微妙に違うのだろう。まあ大勢に影響はないので一旦よしとする。

しかし土日はNNの移植で終わってしまったな……。解法はあまり深く考えられていないので考察を進めないと。木曜が祝日なのが救い。

## 3日目（月）

NNが一段落したので本丸の実装に入っていく。以下のような感じ？

1. 家・水源・その他ランダムな点を適当にボーリング調査する
   1. ボーリング調査の際は、既に調査した点とのマンハッタン距離が閾値以下の箇所は再評価しないようにする
2. 40x40の粗いマップの上で仮の経路を決め、経路上の点を適当な間隔でボーリング調査する
   1. 最小シュタイナー森（？）問題なので、とりあえず以下の方法で解く
      1. 未接続の家を全てキューに入れ、ダイクストラを行う
      2. 水源か、それと連結している坑道に辿り着いたらその家だけ経路を復元する
      3. 未接続の家がなくなるまで1.-2.を繰り返す
   2. 焼いても良いかもしれない。「必ず使わなければならない点」を焼くイメージ

NNで予測するとき、「自信度」みたいなものを出しても良いかもしれないな。メモ。

NNのサイズ小さくしたせいで細部のディテールが甘くなってるな……。ネットワーク構成はちょっと考えた方が良いかも。浅い層のチャンネルが少なすぎるだろうか。

現在のNNのタスクは「40x40マスのマップの中から8～256点だけサンプリングしたものが与えられるので、いい感じに予測してください」というものになっているが、さすがに8点だけで予測しろというのは無理がありそう。そういうインスタンスで大外しをしないようにという気持ちになりそうで、精度的にも悪影響を及ぼしている気がする。32～256点に変えてみる。

予想通り全体的によくなった気がする。小さめのまだら模様もまあまあ復元してくれるようになった。

学習データ作成時に8bitグレースケール画像として保存していたため、全てが256階調に丸められていたことに気付く。アホ。画像は画像で確認時に便利なので残しておくけど、学習用データはnumpyでバイナリ出力したものを使うことにしよう……。

必ず使わなければならない点を決める焼きなましは最小全域木問題になるのだが、これには全点間の距離が必要。ここで必ず使わなければならない点同士で接続しない解に限ることにするとダイクストラの回数が（家の数）＋（水源の数）になるので、距離を求めるパートの計算量がO(N^2 logN)からO((W+K)NlogN)になるな。前者でも大丈夫かもしれないが、メモ。

ついでに焼きなましの更新パートのクラスカル法でも、追加の頂点を考えないケースで最小全域木を計算してあげるとそこで使わない辺は絶対使わないので（だよね？）、追加の頂点数をMとして辺の数を(W+K+M)(W+K+M-1)/2から(W+K-1)+(W+K)Mに落とせるな。そこそこ高速化できそう。
