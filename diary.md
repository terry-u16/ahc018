# AHC018日記

## 1日目（土）

問題を読んだ。面白い。

AHC003にかなり似ているな。コストを推定しながら最小シュタイナー森（水源を1頂点にまとめればそう考えてよい）を構築する問題。

ただAHC003と違って、事前にボーリング調査をやってしまってよい。ボーリング調査をする際は二分探索をするなり平方分割っぽくやる（100階建てのビルから卵を落とすGoogleの例の問題）なりすれば効率的にできそう。どうせ近くにあるマスのコストはそんなに変わらないので近場いくつか掘ってしまってよい。

ボーリング調査結果からコストを復元するのはどうすればいいだろう。Perlin noiseについて調べてみると、Wavelet関数なるものを格子点上にたくさん作ってそれらを補間してあげているらしい。だけどこれを逆方向に推定できる気がしないな。NN使っていい感じに求められないだろうか。超解像とかimg to imgのネットワークとか。

もしNNで求められるようになれば、仮経路を決める→ボーリング調査→仮経路を決める→ボーリング調査→……の繰り返しで少しずつ精緻化していくのがよい気がする。必ずしも1マスずつ順番に掘り進める必要はない。

NNのタスクを定義する。入出力ともに40x40のグレースケール画像とし、出力側は真のマップをリサイズした画像、入力側は出力側からいくつかの点をサンプリングした画像とする。入力側の欠損値はとりあえずサンプリングされた画像の平均値とする。チャンネルを増やして欠損した・していない情報を付加してもよいが、いったん置いておく。

そういえばパラメータ数は提出可能な範囲に収まっているのだろうか。チャンネル数を無闇に増やさなければ大丈夫だと思ってるけど……。

img to imgということでとりあえずUNetを組んでみる。正直全くノウハウがないので「モザイク除去から学ぶ_最先端のディープラーニング」「Kaggleに挑む深層学習プログラミングの極意」などを読みながら必死にそれっぽいものを作った。python難しすぎ……。パラメータ等はあとでRust移植することを考えず適当。

予測してみる。なんだこれ……思った以上にちゃんと予測できてるぞ……！正直ここまでとは思っていなかったので期待以上。かなりテンションが上がっている。いや本当に凄い……。

チャンネルを増やして欠損した・していない情報を付加した。人間の目で見ても分かるくらいに改善されている。これは凄い。苦労してNNを実装した甲斐があった……。

周波数成分が高いケースと低いケースがあるな。下手するとFFTが使えてしまうかもしれない。[AHCで絶対使わないであろうアルゴリズムリスト](https://twitter.com/terry_u16/status/1626768939094274048)に挙げてたのに……。

信頼区間とかを推定するネットワークがあってもいいかもしれない。メモ。

## 2日目（日）

NNのパラメータ数を減らそうかと思ったけど一旦良いか。パラメータ多すぎかと思ったけど減らすと一応目に見えるレベルで出力が変わるので無駄ではないらしい。とはいえSOTA目指すわけじゃないからあとで実行速度・埋め込み可否と相談かな。

白黒の画像を並べて見比べてたらレントゲン写真を見てるお医者さんの気分になってきた。

NNに必要な部品（Conv2D, BatchNormalization, MaxPoolingなど）を移植する。つらい。とりあえず一つずつ丁寧にテストしていく。

部品まで作成し終わったので、息抜きにNNのoptimizerや学習率スケジューリングをいじってみる。思った以上に効く。知識として知っていても自分で実際に触ってみるのとでは大違いだな。

予測結果を眺めてみる。よく推論できている。何が凄いって、サンプリングされた点が近くになくてもそれらしい傾向を出して来る点が凄い。画像全体の周波数成分から拾ってきているのだろうか。これは人間にはできないな。

低周波成分が強いインスタンスには強い一方、高周波成分が強いインスタンスには弱い傾向がありそう。ナイキスト周波数問題のようなそうでないような。モデルを複数個用意するのもアリかもしれない。

パラメータ数を1桁間違えていた（え？）結構カツカツだな……。うまくバランスを取る必要がありそう。まあ細かい詰めは後で良いか。

高周波成分に対応できるようにするためには、入力で欠損していないピクセルだけ損失関数を大きくしてあげると良いかもしれない。とはいえ低周波には弱くなるからモデル2つ作るのがベターか。エンコード時のFP32をFP16に変えると2倍詰め込めるかも。幸いFP16→FP32の変換はそこまで難しくないらしい。

ようやくネットワークの移植が完了。パーツごとにテストした甲斐があってほぼバグなく結果が出てきた。「ほぼ」というのは結果の小数点第3位とかが微妙に違っているため。batch_normalizationのパラメータかパディングか、なんかそういう細かいところが微妙に違うのだろう。まあ大勢に影響はないので一旦よしとする。

しかし土日はNNの移植で終わってしまったな……。解法はあまり深く考えられていないので考察を進めないと。木曜が祝日なのが救い。

## 3日目（月）

なんか潜伏を疑われている。基本的に何も考えず貪欲に出すので、提出していないときは単純に実装が間に合っていないかアイデアが生えないかのどちらかなんだよな。

NNが一段落したので本丸の実装に入っていく。以下のような感じ？

1. 家・水源・その他ランダムな点を適当にボーリング調査する
   1. ボーリング調査の際は、既に調査した点とのマンハッタン距離が閾値以下の箇所は再評価しないようにする
2. 40x40の粗いマップの上で仮の経路を決め、経路上の点を適当な間隔でボーリング調査する
   1. 最小シュタイナー森（？）問題なので、とりあえず以下の方法で解く
      1. 未接続の家を全てキューに入れ、ダイクストラを行う
      2. 水源か、それと連結している坑道に辿り着いたらその家だけ経路を復元する
      3. 未接続の家がなくなるまで1.-2.を繰り返す
   2. 焼いても良いかもしれない。「必ず使わなければならない点」を焼くイメージ

NNで予測するとき、「自信度」みたいなものを出しても良いかもしれないな。メモ。

NNのサイズ小さくしたせいで細部のディテールが甘くなってるな……。ネットワーク構成はちょっと考えた方が良いかも。浅い層のチャンネルが少なすぎるだろうか。

現在のNNのタスクは「40x40マスのマップの中から8～256点だけサンプリングしたものが与えられるので、いい感じに予測してください」というものになっているが、さすがに8点だけで予測しろというのは無理がありそう。そういうインスタンスで大外しをしないようにという気持ちになりそうで、精度的にも悪影響を及ぼしている気がする。32～256点に変えてみる。

予想通り全体的によくなった気がする。小さめのまだら模様もまあまあ復元してくれるようになった。

学習データ作成時に8bitグレースケール画像として保存していたため、全てが256階調に丸められていたことに気付く。アホ。画像は画像で確認時に便利なので残しておくけど、学習用データはnumpyでバイナリ出力したものを使うことにしよう……。

必ず使わなければならない点を決める焼きなましは最小全域木問題になるのだが、これには全点間の距離が必要。ここで必ず使わなければならない点同士で接続しない解に限ることにするとダイクストラの回数が（家の数）＋（水源の数）になるので、距離を求めるパートの計算量がO(N^2 logN)からO((W+K)NlogN)になるな。前者でも大丈夫かもしれないが、メモ。

ついでに焼きなましの更新パートのクラスカル法でも、追加の頂点を考えないケースで最小全域木を計算してあげるとそこで使わない辺は絶対使わないので（だよね？）、追加の頂点数をMとして辺の数を(W+K+M)(W+K+M-1)/2から(W+K-1)+(W+K)Mに落とせるな。そこそこ高速化できそう。

## 4日目（火）

全然眠れなかった。AHC中は脳が興奮してしまってダメ……。

ひたすら実装。実装。実装。とりあえず手を動かしまくる。

実装が終わった。バグもほとんどなし。なんかバグらせにくい書き方を会得しつつあるかもしれない（トヨタコン予選BのC問題のしょうもないバグで本戦出場を逃した人）。

実行してみる。やりたいことはできていて、スコアも2.5倍くらい改善した。が、まだまだ上位には全然足りていない。問題はいくつかあって、

- 調査ボーリングで結構コストがかさむ。硬い岩盤を踏むと情報量も多いのだが、コストもキツい
- CNNの予測が甘い。大まかに予測するのは得意だが、細かいところでズレが生じる

といったところ。パワー調整も甘いのだが、そこを詰めるより先にやることがありそう。予測を正確にやらないとパワー調整もやりようがないし。

## 5日目（水）

ブルーアーカイブのメインストーリーが更新されたので、読みました。

## 6日目（木）

ベイズ最適化とか使えないかなと思って調べた結果、ガウス過程という概念があることを知る。なんかちょっとずつ予測が絞れていくようなgifアニメは見たことあるな。値だけじゃなくて分散（≒確からしさ）まで予測できるのは嬉しそう。[ガウス過程と機械学習](https://amzn.to/3ZelPhs)という本を買って読む。

祝日一日潰してまあまあ苦しんだが、それなりに丁寧に書いてくれていることもあってとりあえずコードを書ける程度にはなった。CNNでの予測と見比べてみる。

うーん、あまり優位性を感じないな。CNNと同じかちょっと劣る程度。分散も一緒に出してくれるのだが、「見てないところは分散が大きい」という自明な情報くらいしか得られなさそう。ハイパーパラメータをグリッドサーチしてたり、カーネルの選択をRBFしか試してなかったりというのもあるのだが、まあそれで劇的に良くなるかというと微妙だよなあ……。今回は使わないけど、まあ新しい知識を得られただけ良いか。技術としてはかなり面白くて、AHC後にじっくり勉強する価値はありそう。

CNNも平均に加えて分散も出してくれると嬉しいかもしれない。損失関数をちゃんと設計すればいける？

## 7日目（金）

ガウス過程、2次元でなくてもダイクストラのパス上で1次元でできたりしない？2次元のケースより簡単そうだしこれなら行けるかも。

前処理ちゃんとした方が良いな。今回の問題の入力生成方法を読むと、ある値を2～4乗したものが頑丈さになるので、とりあえず入力全体に対して平方根を取っておく。あとは平均を取っていなかったので取った（え？）。

結構苦しんだが、どうにかそれらしい予測ができるようになった。ちゃんと±2σくらいにだいたい収まってるの凄いな。そして昨日よりだいぶ理解が深まってきた。

今になって見てみると、昨日やった2次元版のガウス過程は色々と間違っているところが多いな……。修正する。

修正したらかなり良くなった。パッと見CNNと同程度くらいの見た目になっていることに加え、ちゃんと標準偏差もそれらしい値で出せている。これは良いな。逆行列計算が重くてグリッドサーチだとさすがに使えなさそうだけど、ちゃんと共役勾配法なり何なりを実装すれば十分な速度になりそう？サンプル点数が多いとそれも無理だけど、補助変数法とやらを使えばなんとかなるかもしれない。

また、これにより各点について頑丈さの確率分布が分かるので、DPをすることで必要コストの期待値が最も小さい手順が求められそう。今までで頑丈さiだけ削っていて、破壊できていない確率をp_i、次のパワーをP、体力パラメータをCとすると、消費する体力の期待値はp_i(P + C)となる。p_iは数値積分で求めるしかなさそう。P2乗から落ちなさそうなので、適当に状態を丸めるか。

金曜日になってもまともに実装できていない。方針は定まってきたけど、これ実装間に合うのか……？

## 8日目（土）

ガウス過程の速度が全然足りないことに気付いた。1回あたり100msとか平気でかかる。これはちょっと使えない……。

補助変数法を試してみる。確かに速くはなっているが、足りない。ガウス過程は諦めるしかないか……。

諦めてCNN解法で行く。昨日考えていた確率DPをやってみる。といっても分散が得られないので、(実際のコスト - CNNの予測値)が正規分布に従うと大胆仮定してやってみる。

動いた。スコアは……？悪化しとるやんけ！さすがにこの予測じゃ無理があるか……。これはもうダメかもしれない。

未練がましくガウス過程を使って予測して遊んでいたところ、逆行列計算は十分速いことが分かった。遅いのはpythonのforループの箇所っぽい。なんやねん。ともあれこれに賭けてみるか……？

1次元のガウス過程から実装する。Pythonで一度やっているので何をやれば良いかは分かっていて、後はひたすら手を動かす。

実装できたけどなんか微妙な予測結果だな……。と思ってPythonの結果と見比べてみるとなんか違う。目を皿のようにしてようやくバグを見付けた。まあまあいい感じの結果になっていて、実行時間も問題なし。行けるかもしれない。

## 9日目（日）

案の定睡眠に失敗したので朝5時から実装。今度は2次元版を実装する。といってもガウス過程部分は全く同じで良くて、インターフェース部分だけ新しく実装。

できた。こちらも実行時間は問題なし。昨日悩んでいたやつは一体……。

4日振りに投げた。1割くらい改善。もう少し伸びてほしかったけど、4日ぶりに進捗が出ただけでも良いか。ここから気合いで伸ばして行かなければ。

あとはなんやかんや細かい調整をした。ちょっといじるとすぐ改善するのでめちゃくちゃ楽しいパート。まあ1週間分の苦労が詰まっているのでこのくらいのボーナスステージはあってもいい。

こうなるとクラウド実行環境を整備したのが結構効いてくるな。10秒足らずで500ケースの結果が返ってくるのはかなり偉い。

最終的に暫定35位、相対スコア36.7Bでフィニッシュ。
